{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data \n",
    "df = pd.read_csv('data_train.csv', header=0)\n",
    "\n",
    "# print(X.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's do some Exploratory data analysis："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Using pie charts to count the number and proportion of positive and negative samples\n",
    "count = df.iloc[:,-1].value_counts()\n",
    "\n",
    "count.plot(kind='pie', labels=['Negative', 'Positive'], autopct='%1.1f%%', shadow=True)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of each feature：\n",
    "for feature in df.columns:\n",
    "    if feature == 'income':\n",
    "        continue\n",
    "    plt.hist(df[df['income'] == 0][feature], bins=20, alpha=0.5, label='Negative')\n",
    "    plt.hist(df[df['income'] == 1][feature], bins=20,alpha=0.5, label='Positive')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(feature)\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation coefficient and plot the heat map\n",
    "corr = df.corr()\n",
    "\n",
    "sns.heatmap(corr, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the exploratory data analysis, we have become familiar with the basic characteristics of the data. Next, we can start training the model.\n",
    "Before training the model a data type conversion is required to encode string type variables into numeric variables, which is done by the LabelEncoder function of the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create LabelEncoder object\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# 对 X_train 中的每一列进行 Label Encoding\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the encoding is complete, the “fnlwgt” columns need to be feature scaled so that the columns are of essentially the same order of magnitude. The min-max scaling method is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Feature scaling for fnlwgt columns\n",
    "df['fnlwgt'] = scaler.fit_transform(df[['fnlwgt']])*100\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 80% of the dataset as the training dataset and the last 20% as the valid dataset\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "df_train = df[:split_idx]\n",
    "df_valid = df[split_idx:]\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_valid.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data pre-processing, the model is trained using L1 regularization to find the best subset of features for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = df_train.iloc[:, :-1]\n",
    "y_train = df_train.iloc[:, -1] \n",
    "\n",
    "# Instantiate the classifier and set the corresponding hyperparameters\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=0)\n",
    "\n",
    "# Train the model using L1 regularization and select the best subset of features\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print the column names of the best feature subset\n",
    "best_features = []\n",
    "for i in range(len(X_train.columns)):\n",
    "    if clf.coef_[0, i] != 0:\n",
    "        best_features.append(X_train.columns[i]) \n",
    "\n",
    "print(best_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the best feature subset original feature subset is consistent. So the original data can be directly used to train the model.\n",
    "Next, the test set is imported and the same data transformation and feature scaling are performed to start preparing the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data_test.csv', header=0)\n",
    "\n",
    "# feature transformation\n",
    "for col in df_test:\n",
    "    if df_test[col].dtype == 'object':\n",
    "        df_test[col] = encoder.fit_transform(df_test[col].astype(str))\n",
    "\n",
    "# feature scaled\n",
    "df_test['fnlwgt'] = scaler.fit_transform(df_test[['fnlwgt']])*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models using the keras framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "X_train = df_train.iloc[:, :-1]\n",
    "y_train = df_train.iloc[:, -1] \n",
    "X_valid = df_valid.iloc[:, :-1]\n",
    "y_valid = df_valid.iloc[:, -1]\n",
    "X_test = df_test.iloc[:, :-1]\n",
    "y_test = df_test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate models using precision, recall and F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)  \n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras model training is complete. Now to try the model in the pytorch framework and check if the accuracy will improve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# data type transformation:\n",
    "\n",
    "# Convert a Pandas DataFrame to a NumPy array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to PyTorch Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=X_train.shape[1], out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "net = Net()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr_decay=0.01)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Convert data into a tensor\n",
    "        inputs = torch.Tensor(X_train[i:i+batch_size])\n",
    "        labels = torch.Tensor(y_train[i:i+batch_size]).unsqueeze(1)\n",
    "\n",
    "        # Forward propagation, computational loss and backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistical losses\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validate on the validation dataset\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.Tensor(X_valid)\n",
    "        labels = torch.Tensor(y_valid).unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        val_loss = criterion(outputs, labels)\n",
    "        val_preds = outputs.round().squeeze().detach().numpy()\n",
    "        val_labels = labels.squeeze().detach().numpy()\n",
    "        val_accuracy = np.mean(val_preds == val_labels)\n",
    "        val_precision = precision_score(val_labels, val_preds)\n",
    "        val_recall = recall_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {running_loss / (X_train.shape[0] / batch_size):.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1-Score: {val_f1:.4f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the Precision, Recall, F1-score as the indicates of model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = torch.Tensor(X_test)\n",
    "    print(y_test.shape)\n",
    "    labels = torch.Tensor(y_test).unsqueeze(1)\n",
    "    outputs = net(inputs)\n",
    "    print(labels.shape, outputs.shape)\n",
    "    labels = torch.round(labels)\n",
    "    print(labels)\n",
    "    test_loss = criterion(outputs, labels)\n",
    "    test_preds = outputs.round().squeeze().detach().numpy()\n",
    "    test_labels = labels.squeeze().detach().numpy()\n",
    "    test_accuracy = np.mean(test_preds == test_labels)\n",
    "    test_precision = precision_score(test_labels, test_preds)\n",
    "    test_recall = recall_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds)\n",
    "    \n",
    "print('Precision: {:.4f}'.format(test_precision))\n",
    "print('Recall: {:.4f}'.format(test_recall))\n",
    "print('F1-score: {:.4f}'.format(test_f1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the keras model and the pytorch model, it is found that the keras model is relatively superior. So the keras model was derived for ensemble learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "\n",
    "model.save('NeuralNetwork.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to import the model, load_model.() should be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "loaded_model = keras.models.load_model('NeuralNetwork.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
