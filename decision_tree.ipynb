{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree for the human dataset\n",
    "## 1. Data Preprocessing\n",
    "we're going to train a decicison tree model for the dataset and test how good it works.\n",
    "We will use the scikit-learn library for its powerful funtionality. First import the packages and read the prepocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data_train = pd.read_csv('data_train.csv')\n",
    "data_test = pd.read_csv('data_test.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sklearn.tree library cannot deal with attributes in strings, so we first have to convert all the string attributes to integers using LabelEncoder(). 'Education' doesn't need to be converted because it was already converted to interger in the original data as a column called 'educational_num'. Each string value of 'education' corrsponds to a unique interger in 'educational_num'. Repeast this process in both training dataset and testing dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a categorical variable to numerical values\n",
    "encoder = LabelEncoder()\n",
    "data_train['workclass'] = encoder.fit_transform(data_train['workclass'])\n",
    "data_train['marital-status'] = encoder.fit_transform(data_train['marital-status'])\n",
    "data_train['occupation'] = encoder.fit_transform(data_train['occupation'])\n",
    "data_train['relationship'] = encoder.fit_transform(data_train['relationship'])\n",
    "data_train['race'] = encoder.fit_transform(data_train['race'])\n",
    "data_train['gender'] = encoder.fit_transform(data_train['gender'])\n",
    "data_train['native-country'] = encoder.fit_transform(data_train['native-country'])\n",
    "data_test['workclass'] = encoder.fit_transform(data_test['workclass'])\n",
    "data_test['marital-status'] = encoder.fit_transform(data_test['marital-status'])\n",
    "data_test['occupation'] = encoder.fit_transform(data_test['occupation'])\n",
    "data_test['relationship'] = encoder.fit_transform(data_test['relationship'])\n",
    "data_test['race'] = encoder.fit_transform(data_test['race'])\n",
    "data_test['gender'] = encoder.fit_transform(data_test['gender'])\n",
    "data_test['native-country'] = encoder.fit_transform(data_test['native-country'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 13 colums as the X, so create X_train and X_test using all these columns. Set y_train and y_test using the 'income' column. As Decision trees are not sensitive to the scale of the features, standard scaling is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  workclass  fnlwgt  educational_num  marital-status  occupation  \\\n",
      "0       39          6   77516               13               4           0   \n",
      "1       50          5   83311               13               2           3   \n",
      "2       38          3  215646                9               0           5   \n",
      "3       53          3  234721                7               2           5   \n",
      "4       28          3  338409               13               2           9   \n",
      "...    ...        ...     ...              ...             ...         ...   \n",
      "32556   27          3  257302               12               2          12   \n",
      "32557   40          3  154374                9               2           6   \n",
      "32558   58          3  151910                9               6           0   \n",
      "32559   22          3  201490                9               4           0   \n",
      "32560   52          4  287927                9               2           3   \n",
      "\n",
      "       relationship  race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
      "0                 1     4       1          2174             0              40   \n",
      "1                 0     4       1             0             0              13   \n",
      "2                 1     4       1             0             0              40   \n",
      "3                 0     2       1             0             0              40   \n",
      "4                 5     2       0             0             0              40   \n",
      "...             ...   ...     ...           ...           ...             ...   \n",
      "32556             5     4       0             0             0              38   \n",
      "32557             0     4       1             0             0              40   \n",
      "32558             4     4       0             0             0              40   \n",
      "32559             3     4       1             0             0              20   \n",
      "32560             5     4       0         15024             0              40   \n",
      "\n",
      "       native-country  \n",
      "0                  38  \n",
      "1                  38  \n",
      "2                  38  \n",
      "3                  38  \n",
      "4                   4  \n",
      "...               ...  \n",
      "32556              38  \n",
      "32557              38  \n",
      "32558              38  \n",
      "32559              38  \n",
      "32560              38  \n",
      "\n",
      "[32561 rows x 13 columns]\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "32556    0\n",
      "32557    1\n",
      "32558    0\n",
      "32559    0\n",
      "32560    1\n",
      "Name: income, Length: 32561, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set x_train and X_test to contain all the input columns of the DataFrame\n",
    "\n",
    "feature_names = ['age', 'workclass', 'fnlwgt', 'educational_num', 'marital-status', 'occupation', 'relationship',\n",
    "                 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
    "\n",
    "x_train = data_train[feature_names]\n",
    "x_test = data_test[feature_names]\n",
    "\n",
    "# Set y_train and y_test to contain the target variable of the DataFrame\n",
    "y_train = data_train['income']\n",
    "y_test = data_test['income']\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. get a prelimenery result using default settings of sklearn.tree\n",
    "feed the data into DecisionTreeClassifier() and get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8068300472943922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87     12435\n",
      "           1       0.59      0.61      0.60      3846\n",
      "\n",
      "    accuracy                           0.81     16281\n",
      "   macro avg       0.73      0.74      0.74     16281\n",
      "weighted avg       0.81      0.81      0.81     16281\n",
      "\n",
      "0.7389380472685735\n",
      "[[10789  1646]\n",
      " [ 1499  2347]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Initialize a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "#Summarize Result\n",
    "#precision,recall,f1-score,support, accuracy, macro avg, weighted avg\n",
    "print(classification_report(y_test,y_pred))\n",
    "#ROC score\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "#confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just the first attempt without validation. The tree we obtain here is too complex. In order to avoid overfitting on training data, validation is necessary. We will start with the most basic hyperparameter, the map depth of the tree. To do so, we first import more packages for the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing validation, we have to split the original training set into the actual training set and a new validation set, while keeping the test set unseen still. So we rename the data directly read from the csv file to data_train_val for clarification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_val = pd.read_csv('data_train.csv')\n",
    "data_train_val['workclass'] = encoder.fit_transform(data_train_val['workclass'])\n",
    "data_train_val['marital-status'] = encoder.fit_transform(data_train_val['marital-status'])\n",
    "data_train_val['occupation'] = encoder.fit_transform(data_train_val['occupation'])\n",
    "data_train_val['relationship'] = encoder.fit_transform(data_train_val['relationship'])\n",
    "data_train_val['race'] = encoder.fit_transform(data_train_val['race'])\n",
    "data_train_val['gender'] = encoder.fit_transform(data_train_val['gender'])\n",
    "data_train_val['native-country'] = encoder.fit_transform(data_train_val['native-country'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the training data and validation data using 5-fold. Print out the best maximum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into real training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_train_val[feature_names], data_train_val['income'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define range of values for maximum depth of tree\n",
    "max_depth_values = range(1, len(feature_names))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over values of max_depth to find the one that makes the highest cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Loop over values of max_depth and calculate cross-validation score for each\n",
    "cv_scores = []\n",
    "for max_depth in max_depth_values:\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Choose value of max_depth that gives best cross-validation score\n",
    "best_max_depth = max_depth_values[np.argmax(cv_scores)]\n",
    "\n",
    "print(best_max_depth)\n",
    "\n",
    "# Train final model using best value of max_depth\n",
    "final_clf = DecisionTreeClassifier(max_depth=best_max_depth)\n",
    "final_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance on validation set\n",
    "val_score = final_clf.score(X_val, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the best_max_depth obtained, make another fitting and check the predition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8569498188071986\n"
     ]
    }
   ],
   "source": [
    "# Test the tree using the test set\n",
    "x_train = data_train_val[feature_names]\n",
    "x_test = data_test[feature_names]\n",
    "\n",
    "# Set y_train and y_test to contain the target variable of the DataFrame\n",
    "y_train = data_train_val['income']\n",
    "y_test = data_test['income']\n",
    "\n",
    "# Initialize a decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=best_max_depth)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the tree, the tree should look like this '![Alt text](decision_tree1.png)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.png'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the decision tree\n",
    "export_graphviz(clf, out_file=\"tree.dot\", feature_names=feature_names, filled=True,\n",
    "                rounded=True, special_characters=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "graph = Source(dot_graph)\n",
    "graph.format = \"png\"\n",
    "graph.render(\"decision_tree\", view=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have significantly shrinked the tree the final accuracy on the test set has improved, which indicates the cross-validation has worked out. Now the question is, is there a need to validate more hyperparameters to further improve the tree? There are several more parameters to adjust, so we start with the pair maximum depth and min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 8, 'min_samples_leaf': 3}\n",
      "Best cross-validation score: 0.8551028211956355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'max_depth': [6, 7, 8, 9],\n",
    "    'min_samples_leaf': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Create a GridSearchCV object and fit it to the data\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "grid_search.fit(data_train_val[feature_names], data_train_val['income'])\n",
    "\n",
    "# Print the best combination of hyperparameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Print the best mean cross-validation score\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the hyperparameters obtained to run the test again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8551686014372581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'decision_tree.png'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tree using the test set\n",
    "x_train = data_train_val[feature_names]\n",
    "x_test = data_test[feature_names]\n",
    "\n",
    "# Set y_train and y_test to contain the target variable of the DataFrame\n",
    "y_train = data_train_val['income']\n",
    "y_test = data_test['income']\n",
    "\n",
    "# Initialize a decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=grid_search.best_params_['max_depth'], min_samples_leaf=grid_search.best_params_['min_samples_leaf'])\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Visualize the decision tree\n",
    "export_graphviz(clf, out_file=\"tree.dot\", feature_names=feature_names, filled=True,\n",
    "                rounded=True, special_characters=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "graph = Source(dot_graph)\n",
    "graph.format = \"png\"\n",
    "graph.render(\"decision_tree\", view=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
