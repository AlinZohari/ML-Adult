{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree for the human dataset\n",
    "## 1. Data Preprocessing\n",
    "we're going to train a decicison tree model for the dataset and test how good it works.\n",
    "We will use the scikit-learn library for its powerful funtionality. First import the packages and read the prepocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data_train = pd.read_csv('data_train.csv')\n",
    "data_test = pd.read_csv('data_test.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sklearn.tree library cannot deal with attributes in strings, so we first have to convert all the string attributes to integers using LabelEncoder(). 'Education' doesn't need to be converted because it was already converted to interger in the original data as a column called 'educational_num'. Each string value of 'education' corrsponds to a unique interger in 'educational_num'. Repeast this process in both training dataset and testing dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a categorical variable to numerical values\n",
    "encoder = LabelEncoder()\n",
    "data_train['workclass'] = encoder.fit_transform(data_train['workclass'])\n",
    "data_train['marital-status'] = encoder.fit_transform(data_train['marital-status'])\n",
    "data_train['occupation'] = encoder.fit_transform(data_train['occupation'])\n",
    "data_train['relationship'] = encoder.fit_transform(data_train['relationship'])\n",
    "data_train['race'] = encoder.fit_transform(data_train['race'])\n",
    "data_train['gender'] = encoder.fit_transform(data_train['gender'])\n",
    "data_train['native-country'] = encoder.fit_transform(data_train['native-country'])\n",
    "data_test['workclass'] = encoder.fit_transform(data_test['workclass'])\n",
    "data_test['marital-status'] = encoder.fit_transform(data_test['marital-status'])\n",
    "data_test['occupation'] = encoder.fit_transform(data_test['occupation'])\n",
    "data_test['relationship'] = encoder.fit_transform(data_test['relationship'])\n",
    "data_test['race'] = encoder.fit_transform(data_test['race'])\n",
    "data_test['gender'] = encoder.fit_transform(data_test['gender'])\n",
    "data_test['native-country'] = encoder.fit_transform(data_test['native-country'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 13 colums as the X, so create X_train and X_test using all these columns. Set y_train and y_test using the 'income' column. As Decision trees are not sensitive to the scale of the features, standard scaling is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  workclass  fnlwgt  educational_num  marital-status  occupation  \\\n",
      "0       39          6   77516               13               4           0   \n",
      "1       50          5   83311               13               2           3   \n",
      "2       38          3  215646                9               0           5   \n",
      "3       53          3  234721                7               2           5   \n",
      "4       28          3  338409               13               2           9   \n",
      "...    ...        ...     ...              ...             ...         ...   \n",
      "32556   27          3  257302               12               2          12   \n",
      "32557   40          3  154374                9               2           6   \n",
      "32558   58          3  151910                9               6           0   \n",
      "32559   22          3  201490                9               4           0   \n",
      "32560   52          4  287927                9               2           3   \n",
      "\n",
      "       relationship  race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
      "0                 1     4       1          2174             0              40   \n",
      "1                 0     4       1             0             0              13   \n",
      "2                 1     4       1             0             0              40   \n",
      "3                 0     2       1             0             0              40   \n",
      "4                 5     2       0             0             0              40   \n",
      "...             ...   ...     ...           ...           ...             ...   \n",
      "32556             5     4       0             0             0              38   \n",
      "32557             0     4       1             0             0              40   \n",
      "32558             4     4       0             0             0              40   \n",
      "32559             3     4       1             0             0              20   \n",
      "32560             5     4       0         15024             0              40   \n",
      "\n",
      "       native-country  \n",
      "0                  38  \n",
      "1                  38  \n",
      "2                  38  \n",
      "3                  38  \n",
      "4                   4  \n",
      "...               ...  \n",
      "32556              38  \n",
      "32557              38  \n",
      "32558              38  \n",
      "32559              38  \n",
      "32560              38  \n",
      "\n",
      "[32561 rows x 13 columns]\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "32556    0\n",
      "32557    1\n",
      "32558    0\n",
      "32559    0\n",
      "32560    1\n",
      "Name: income, Length: 32561, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set x_train and X_test to contain all the input columns of the DataFrame\n",
    "\n",
    "feature_names = ['age', 'workclass', 'fnlwgt', 'educational_num', 'marital-status', 'occupation', 'relationship',\n",
    "                 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
    "\n",
    "x_train = data_train[feature_names]\n",
    "x_test = data_test[feature_names]\n",
    "\n",
    "# Set y_train and y_test to contain the target variable of the DataFrame\n",
    "y_train = data_train['income']\n",
    "y_test = data_test['income']\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. get a prelimenery result using default settings of sklearn.tree\n",
    "feed the data into DecisionTreeClassifier() and get the result. The accuracy is 0.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87     12435\n",
      "           1       0.59      0.61      0.60      3846\n",
      "\n",
      "    accuracy                           0.81     16281\n",
      "   macro avg       0.73      0.74      0.74     16281\n",
      "weighted avg       0.81      0.81      0.81     16281\n",
      "\n",
      "0.7386096939655632\n",
      "[[10797  1638]\n",
      " [ 1504  2342]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Initialize a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "#Summarize Result\n",
    "#precision,recall,f1-score,support, accuracy, macro avg, weighted avg\n",
    "print(classification_report(y_test,y_pred))\n",
    "#ROC score\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "#confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning\n",
    "### 3.1 Preparation for tuning\n",
    "Above was just the first attempt without validation. The tree we obtain here is too complex. In order to avoid overfitting on training data, validation is necessary. We will start with the most basic hyperparameter, the map depth of the tree. Loop over values of max_depth to find the one that makes the highest cross-validation score. There are several metrics for evaluating a decision tree model, such as accuracy, precision, recall and AUC. They can be useful depending on the specific problem. Since the data is quite imbalanced, we believe the AUC (Area Under the ROC Curve) should be the most useful one but we will list all the other metrics when comparing the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunyu\\anaconda3\\envs\\geospatial\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sunyu\\anaconda3\\envs\\geospatial\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sunyu\\anaconda3\\envs\\geospatial\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sunyu\\anaconda3\\envs\\geospatial\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sunyu\\anaconda3\\envs\\geospatial\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: 8\n",
      "Accuracy: 0.854887842911795\n",
      "Precision: 0.7814208412563405\n",
      "Recall: 0.5518451405418765\n",
      "AUC: 0.8983571492400083\n",
      "|    |   fit_time |   score_time |   test_accuracy |   test_precision |   test_recall |   test_roc_auc |\n",
      "|---:|-----------:|-------------:|----------------:|-----------------:|--------------:|---------------:|\n",
      "|  0 |  0.104717  |    0.0139616 |        0.847075 |         0.703047 |      0.63225  |       0.87409  |\n",
      "|  1 |  0.0917549 |    0.0139959 |        0.847359 |         0.703546 |      0.632653 |       0.870982 |\n",
      "|  2 |  0.0987043 |    0.0149584 |        0.851505 |         0.707099 |      0.654337 |       0.867855 |\n",
      "|  3 |  0.0937514 |    0.0149603 |        0.85734  |         0.739326 |      0.629464 |       0.881872 |\n",
      "|  4 |  0.0977356 |    0.0139642 |        0.849355 |         0.729118 |      0.595663 |       0.873141 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Define range of values for maximum depth of tree\n",
    "max_depth_values = range(1, len(feature_names))\n",
    "\n",
    "# Create arrays to store the evaluation metrics for each max_depth value\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "\n",
    "    # Create a new DecisionTreeClassifier with the current max_depth value\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "\n",
    "    # Use cross_validate to calculate the evaluation metrics with 5-fold cross-validation\n",
    "    cv_results = cross_validate(clf, x_train, y_train, cv=5, scoring=['accuracy', 'precision', 'recall', 'roc_auc'])\n",
    "\n",
    "    # Store the mean evaluation metrics for the current max_depth value\n",
    "    accuracy_scores.append(np.mean(cv_results['test_accuracy']))\n",
    "    precision_scores.append(np.mean(cv_results['test_precision']))\n",
    "    recall_scores.append(np.mean(cv_results['test_recall']))\n",
    "    auc_scores.append(np.mean(cv_results['test_roc_auc']))\n",
    "\n",
    "# Find the index of the max AUC score\n",
    "best_index = np.argmax(auc_scores)\n",
    "\n",
    "# Find the best max_depth value based on the index\n",
    "best_max_depth = max_depth_values[best_index]\n",
    "\n",
    "# Print the best max_depth value and the corresponding evaluation metrics\n",
    "print(\"Best max_depth:\", best_max_depth)\n",
    "print(\"Accuracy:\", accuracy_scores[best_index])\n",
    "print(\"Precision:\", precision_scores[best_index])\n",
    "print(\"Recall:\", recall_scores[best_index])\n",
    "print(\"AUC:\", auc_scores[best_index])\n",
    "\n",
    "# Covert the array to a Dataframe\n",
    "df_cv_results = pd.DataFrame(cv_results, columns=['fit_time', 'score_time', 'test_accuracy','test_precision','test_recall','test_roc_auc'])\n",
    "print(df_cv_results.to_markdown())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the maximum depth 8 is found to be the best maximum of depth."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Further validation: grid search of hyperparameters \n",
    "Now that we have made our first step into validation, the question becomes, is there a need to validate more hyperparameters to further improve the tree? It's very obvious that the more parameters we put into validation, the higher validation score we will get, but this doesn't necessarily mean the model becomes better because we may end up overfitting to the validation set. Thankfully the fold validation method can somehow cope with this problem. However since the validation set is chosen randomly, the result is not always reproducable. In this case, we start with a broad range of parameters and repeat severl times to narrow down the range we are interest in and then continue on finer ranges. We start with the range below. We first ran the cross-validation 10 times before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9023159967690738\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 15, 'splitter': 'best'}\n",
      "Best score:  0.9020759235497507\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 6, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9018326918331608\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 15, 'splitter': 'best'}\n",
      "Best score:  0.9026765383844827\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 6, 'min_samples_leaf': 10, 'min_samples_split': 15, 'splitter': 'best'}\n",
      "Best score:  0.90245374058493\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9030600977787238\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 15, 'splitter': 'best'}\n",
      "Best score:  0.9015087997913884\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9022699344914443\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'entropy', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9028009257856118\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9029742293622496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter space for the grid search\n",
    "param_grid = {\n",
    "    'max_depth': [6, 8, 10, 12],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [2, 4, 8, 10],\n",
    "    'max_features': [4, 6, 8],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and their score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "    print(\"Best score: \", best_score)\n",
    "\n",
    "    # print the result into a file each time\n",
    "    with open('output.txt', 'a') as f:\n",
    "        print(\"Best hyperparameters: \", best_params, file=f)\n",
    "        print(\"Best score: \", best_score, file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results of the cross validation:\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9018088535869502\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9023159967690738\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 15, 'splitter': 'best'}\n",
    "Best score:  0.9020759235497507\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 6, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9018326918331608\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 15, 'splitter': 'best'}\n",
    "Best score:  0.9026765383844827\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 6, 'min_samples_leaf': 10, 'min_samples_split': 15, 'splitter': 'best'}\n",
    "Best score:  0.90245374058493\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "Best score:  0.9030600977787238\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 15, 'splitter': 'best'}\n",
    "Best score:  0.9015087997913884\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9022699344914443\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'entropy', 'max_depth': 8, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9028009257856118\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "Best score:  0.9029742293622496\n",
    "\n",
    "Among the hyperparameters, 'class_weight' and 'splitter' return consistent results, so their values can be fixed. 'max_depth', 'max_features' and 'min_samples_leaf' always switch between two closest values, so their serchaing ranges can be narrowed down. However, min_samples_split doesn't show any pattern, so we keep the original inverval for it. Now we continue with the sencond stage of puning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9030214567701511\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9030683462814768\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9034754487608545\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 9, 'min_samples_split': 10, 'splitter': 'best'}\n",
      "Best score:  0.9031170104918192\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'entropy', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 7, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.902907821853205\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9040258717842488\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 10, 'splitter': 'best'}\n",
      "Best score:  0.9033427207923024\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 10, 'min_samples_split': 15, 'splitter': 'best'}\n",
      "Best score:  0.903702840528228\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 7, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9029502519913434\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.903802099232124\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter space for the grid search\n",
    "param_grid = {\n",
    "    'max_depth': [7, 8, 9, 10, 11],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [7, 8, 9, 10, 11],\n",
    "    'max_features': [5, 6, 7, 8],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'class_weight': [None],\n",
    "    'splitter': ['best']\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and their score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "    print(\"Best score: \", best_score)\n",
    "\n",
    "    # print the result into a file each time\n",
    "    with open('output2.txt', 'a') as f:\n",
    "        print(\"Best hyperparameters: \", best_params, file=f)\n",
    "        print(\"Best score: \", best_score, file=f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a look at the second attempt of puning: Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9030214567701511\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "Best score:  0.9030683462814768\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9034754487608545\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 9, 'min_samples_split': 10, 'splitter': 'best'}\n",
    "Best score:  0.9031170104918192\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'entropy', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 7, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.902907821853205\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9040258717842488\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 10, 'splitter': 'best'}\n",
    "Best score:  0.9033427207923024\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 10, 'min_samples_split': 15, 'splitter': 'best'}\n",
    "Best score:  0.903702840528228\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 7, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "Best score:  0.9029502519913434\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "Best score:  0.903802099232124\n",
    "\n",
    "\n",
    "'criterion' returns as 'gini' 9 out of 10 times, so we will fix it as 'gini'. 'max_depth' returns 9 or 10, so this will be the next range for it. 'max_features' returns 7 or 8, so the range can also be narrowed down. 'min_samples_leaf' and 'min_samples_split' don't show much improvement, so in the next stage, they will become the main focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9037045168895492\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 10, 'splitter': 'best'}\n",
      "Best score:  0.9038137829852172\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 16, 'splitter': 'best'}\n",
      "Best score:  0.903599677396058\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9038987503057883\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 10, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9039794703028206\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 16, 'splitter': 'best'}\n",
      "Best score:  0.9030998559778391\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9033415242064644\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 14, 'splitter': 'best'}\n",
      "Best score:  0.9029601854222502\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9036369179988935\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 8, 'splitter': 'best'}\n",
      "Best score:  0.9043456265969627\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter space for the grid search\n",
    "param_grid = {\n",
    "    'max_depth': [9, 10],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16],\n",
    "    'min_samples_leaf': [7, 8, 9, 10, 11],\n",
    "    'max_features': [7, 8],\n",
    "    'criterion': ['gini'],\n",
    "    'class_weight': [None],\n",
    "    'splitter': ['best']\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and their score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "    print(\"Best score: \", best_score)\n",
    "\n",
    "    # print the result into a file each time\n",
    "    with open('output3.txt', 'a') as f:\n",
    "        print(\"Best hyperparameters: \", best_params, file=f)\n",
    "        print(\"Best score: \", best_score, file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third stage, we got this result:\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9037045168895492\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 10, 'splitter': 'best'}\n",
    "Best score:  0.9038137829852172\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 16, 'splitter': 'best'}\n",
    "Best score:  0.903599677396058\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9038987503057883\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 7, 'min_samples_leaf': 10, 'min_samples_split': 4, 'splitter': 'best'}\n",
    "Best score:  0.9039794703028206\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 16, 'splitter': 'best'}\n",
    "Best score:  0.9030998559778391\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 4, 'splitter': 'best'}\n",
    "Best score:  0.9033415242064644\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 14, 'splitter': 'best'}\n",
    "Best score:  0.9029601854222502\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9036369179988935\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 8, 'splitter': 'best'}\n",
    "Best score:  0.9043456265969627\n",
    "\n",
    "Now 'max_depth' is determind at 9 and 'max_features' at 8. 'min_samples_split' still varies a lot. In the next step, we will increase the number of folds to see whether it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 12, 'splitter': 'best'}\n",
      "Best score:  0.9041449909728595\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9038166181826603\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 8, 'splitter': 'best'}\n",
      "Best score:  0.9037586841597687\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 6, 'splitter': 'best'}\n",
      "Best score:  0.9039576452353714\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 7, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9040563125410946\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 8, 'splitter': 'best'}\n",
      "Best score:  0.9044134180305449\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 8, 'splitter': 'best'}\n",
      "Best score:  0.9046020616791333\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 7, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.9049439115861855\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 10, 'splitter': 'best'}\n",
      "Best score:  0.9043810854454328\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 6, 'splitter': 'best'}\n",
      "Best score:  0.9047370751625274\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter space for the grid search\n",
    "param_grid = {\n",
    "    'max_depth': [9],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16],\n",
    "    'min_samples_leaf': [7, 8, 9, 10, 11],\n",
    "    'max_features': [8],\n",
    "    'criterion': ['gini'],\n",
    "    'class_weight': [None],\n",
    "    'splitter': ['best']\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10, scoring='roc_auc')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and their score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "    print(\"Best score: \", best_score)\n",
    "\n",
    "    # print the result into a file each time\n",
    "    with open('output4.txt', 'a') as f:\n",
    "        print(\"Best hyperparameters: \", best_params, file=f)\n",
    "        print(\"Best score: \", best_score, file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result looks like this:Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 12, 'splitter': 'best'}\n",
    "Best score:  0.9041449909728595\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 4, 'splitter': 'best'}\n",
    "Best score:  0.9038166181826603\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 8, 'splitter': 'best'}\n",
    "Best score:  0.9037586841597687\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 6, 'splitter': 'best'}\n",
    "Best score:  0.9039576452353714\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 7, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9040563125410946\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 8, 'splitter': 'best'}\n",
    "Best score:  0.9044134180305449\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 8, 'splitter': 'best'}\n",
    "Best score:  0.9046020616791333\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 7, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "Best score:  0.9049439115861855\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 10, 'splitter': 'best'}\n",
    "Best score:  0.9043810854454328\n",
    "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 6, 'splitter': 'best'}\n",
    "Best score:  0.9047370751625274\n",
    "\n",
    "The range of min_samples_split seems to narrow down a little to smaller numbers so we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9050972547937919\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 10, 'splitter': 'best'}\n",
      "Best score:  0.9040036813454588\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 9, 'splitter': 'best'}\n",
      "Best score:  0.9043603160414188\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9037581529272398\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9041530376798855\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "Best score:  0.9048167342427046\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 6, 'splitter': 'best'}\n",
      "Best score:  0.9043858325884772\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 7, 'splitter': 'best'}\n",
      "Best score:  0.9044117874253776\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9043367763669071\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 9, 'splitter': 'best'}\n",
      "Best score:  0.9044624468097021\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 7, 'min_samples_split': 9, 'splitter': 'best'}\n",
      "Best score:  0.9038662465867555\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9039982569170919\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 9, 'splitter': 'best'}\n",
      "Best score:  0.9038349548085576\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 7, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9048735832550354\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9040020267487213\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 7, 'splitter': 'best'}\n",
      "Best score:  0.9042901499984539\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9040687111546895\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 6, 'splitter': 'best'}\n",
      "Best score:  0.9042322744755561\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 11, 'min_samples_split': 6, 'splitter': 'best'}\n",
      "Best score:  0.9044075596834027\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9038961956586327\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter space for the grid search\n",
    "param_grid = {\n",
    "    'max_depth': [9],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'min_samples_leaf': [7, 8, 9, 10, 11],\n",
    "    'max_features': [8],\n",
    "    'criterion': ['gini'],\n",
    "    'class_weight': [None],\n",
    "    'splitter': ['best']\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(1,21):\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10, scoring='roc_auc')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and their score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "    print(\"Best score: \", best_score)\n",
    "\n",
    "    # print the result into a file each time\n",
    "    with open('output5.txt', 'a') as f:\n",
    "        print(\"Best hyperparameters: \", best_params, file=f)\n",
    "        print(\"Best score: \", best_score, file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result, further narrow down the ranges and increase the fold number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 8, 'splitter': 'best'}\n",
      "Best score:  0.9042196046938257\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 9, 'splitter': 'best'}\n",
      "Best score:  0.9046673402911278\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 9, 'min_samples_split': 6, 'splitter': 'best'}\n",
      "Best score:  0.9049676922110279\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9045800591908494\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 8, 'min_samples_split': 6, 'splitter': 'best'}\n",
      "Best score:  0.9042498109742552\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 8, 'splitter': 'best'}\n",
      "Best score:  0.9036321600439752\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9042811406010813\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "Best score:  0.9045426143109732\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 4, 'splitter': 'best'}\n",
      "Best score:  0.9045592250277418\n",
      "Best hyperparameters:  {'class_weight': None, 'criterion': 'gini', 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 8, 'splitter': 'best'}\n",
      "Best score:  0.9037348391038151\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter space for the grid search\n",
    "param_grid = {\n",
    "    'max_depth': [9],\n",
    "    'min_samples_split': [4, 5, 6, 7, 8, 9],\n",
    "    'min_samples_leaf': [8, 9, 10],\n",
    "    'max_features': [8],\n",
    "    'criterion': ['gini'],\n",
    "    'class_weight': [None],\n",
    "    'splitter': ['best']\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=20, scoring='roc_auc')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and their score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "    print(\"Best score: \", best_score)\n",
    "\n",
    "    # print the result into a file each time\n",
    "    with open('output6.txt', 'a') as f:\n",
    "        print(\"Best hyperparameters: \", best_params, file=f)\n",
    "        print(\"Best score: \", best_score, file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the best score is not sensitive with the hyperparameters anymore, we choose the modes for 'min_samples_leaf'(10) and 'min_samples_split'(8) as the final values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Use the test set to get the final result\n",
    "use the hyperparameters obtained to run the test again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8565812910754868\n",
      "Precision: 0.7762340036563071\n",
      "Recall: 0.5520020800832033\n",
      "AUC: 0.7513930786423254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'decision_tree.png'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tree using the test set\n",
    "x_train = data_train_val[feature_names]\n",
    "x_test = data_test[feature_names]\n",
    "\n",
    "# Set y_train and y_test to contain the target variable of the DataFrame\n",
    "y_train = data_train_val['income']\n",
    "y_test = data_test['income']\n",
    "\n",
    "# Initialize a decision tree classifier\n",
    "clf = DecisionTreeClassifier(class_weight=None, criterion= 'gini', max_depth=9, max_features= 8, min_samples_leaf=10, min_samples_split= 8, splitter= 'best')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the decision tree\n",
    "export_graphviz(clf, out_file=\"tree.dot\", feature_names=feature_names, filled=True,\n",
    "                rounded=True, special_characters=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "graph = Source(dot_graph)\n",
    "graph.format = \"png\"\n",
    "graph.render(\"decision_tree\", view=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
