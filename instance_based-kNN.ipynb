{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance-Based Learning: K-Nearest Neighbours(kNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#scikit-learn library - not complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data_train.csv')\n",
    "test = pd.read_csv('data_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation of the categorical attrbutes to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Select the categorical columns to encode\n",
    "cat_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native-country\"]\n",
    "\n",
    "# Encode categorical columns using Label Encoding for data_train\n",
    "le = LabelEncoder()\n",
    "for col in cat_columns:\n",
    "    train[col] = le.fit_transform(train[col])\n",
    "\n",
    "# Encode categorical columns using Label Encoding for data_test\n",
    "le = LabelEncoder()\n",
    "for col in cat_columns:\n",
    "    test[col] = le.fit_transform(test[col])\n",
    "\n",
    "\n",
    "# Print the first 5 rows of the transformed dataset\n",
    "#print(train.head())\n",
    "#print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  workclass  fnlwgt  education  educational_num  marital-status  \\\n",
      "0       39          6   77516          9               13               4   \n",
      "1       50          5   83311          9               13               2   \n",
      "2       38          3  215646         11                9               0   \n",
      "3       53          3  234721          1                7               2   \n",
      "4       28          3  338409          9               13               2   \n",
      "...    ...        ...     ...        ...              ...             ...   \n",
      "32556   27          3  257302          7               12               2   \n",
      "32557   40          3  154374         11                9               2   \n",
      "32558   58          3  151910         11                9               6   \n",
      "32559   22          3  201490         11                9               4   \n",
      "32560   52          4  287927         11                9               2   \n",
      "\n",
      "       occupation  relationship  race  gender  capital-gain  capital-loss  \\\n",
      "0               0             1     4       1          2174             0   \n",
      "1               3             0     4       1             0             0   \n",
      "2               5             1     4       1             0             0   \n",
      "3               5             0     2       1             0             0   \n",
      "4               9             5     2       0             0             0   \n",
      "...           ...           ...   ...     ...           ...           ...   \n",
      "32556          12             5     4       0             0             0   \n",
      "32557           6             0     4       1             0             0   \n",
      "32558           0             4     4       0             0             0   \n",
      "32559           0             3     4       1             0             0   \n",
      "32560           3             5     4       0         15024             0   \n",
      "\n",
      "       hours-per-week  native-country  \n",
      "0                  40              38  \n",
      "1                  13              38  \n",
      "2                  40              38  \n",
      "3                  40              38  \n",
      "4                  40               4  \n",
      "...               ...             ...  \n",
      "32556              38              38  \n",
      "32557              40              38  \n",
      "32558              40              38  \n",
      "32559              20              38  \n",
      "32560              40              38  \n",
      "\n",
      "[32561 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train\n",
    "x_train_trans = train.drop(columns = ['income'])\n",
    "y_train = train['income']\n",
    "\n",
    "#test\n",
    "x_test_trans = test.drop(columns =['income'])\n",
    "y_test = test['income']\n",
    "\n",
    "print(x_train_trans)\n",
    "#print(y_train_trans)\n",
    "#print(x_test_trans)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Scaling\n",
    "\n",
    "\n",
    "StandardScaler is being used to standardize the data by removing the mean and scaling to unit variance. This is commn preprocessing step in machine learning to ensure that all features are on the same scale, which can improve the performance and accuracy of many algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99412926 -0.09851079  0.35347399 ... -0.21806206 -0.03143184\n",
      "   0.25775643]\n",
      " [-0.05541716 -0.09851079 -0.94239062 ... -0.21806206  0.7699177\n",
      "   0.25775643]\n",
      " [-0.77750339 -1.88752825  1.39544986 ... -0.21806206 -0.03143184\n",
      "   0.25775643]\n",
      " ...\n",
      " [-0.05541716 -0.09851079  1.75522095 ... -0.21806206  0.7699177\n",
      "   0.25775643]\n",
      " [ 0.37783458 -0.09851079 -0.99842039 ... -0.21806206 -0.03143184\n",
      "   0.25775643]\n",
      " [-0.27204303  0.79599794 -0.0689392  ... -0.21806206  1.57126723\n",
      "   0.25775643]]\n"
     ]
    }
   ],
   "source": [
    "#Standard Scaling\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x_train = preprocessing.StandardScaler().fit(x_train_trans).transform(x_train_trans.astype(float))\n",
    "#print(x_train)\n",
    "\n",
    "x_test = preprocessing.StandardScaler().fit(x_test_trans).transform(x_test_trans.astype(float))\n",
    "print(x_test)\n",
    "\n",
    "#x_test have nan value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using default kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89     12435\n",
      "           1       0.65      0.57      0.61      3846\n",
      "\n",
      "    accuracy                           0.83     16281\n",
      "   macro avg       0.76      0.74      0.75     16281\n",
      "weighted avg       0.82      0.83      0.82     16281\n",
      "\n",
      "0.7405577019220697\n",
      "[[11269  1166]\n",
      " [ 1635  2211]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "#base model\n",
    "default_knn = KNeighborsClassifier()\n",
    "default_knn.fit(x_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred=default_knn.predict(x_test)\n",
    "#Error:Input X contains NaN\n",
    "\n",
    "\n",
    "#Summarize Result\n",
    "#precision,recall,f1-score,support, accuracy, macro avg, weighted avg\n",
    "print(classification_report(y_test,y_pred))\n",
    "#ROC score\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "#confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a validation of performance and confusion matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning for K-neighbours, Weight and Distance Metric and validating its performance using accuracy, precision, recall and F1-score\n",
    "\n",
    "This will be done by making a ranking of validation\n",
    "\n",
    "-GridSearchCV \n",
    "with a sklearn.model_selection.GridSearchCV\n",
    "\n",
    "-Training and Validation Split 10% \n",
    "    -goals on validation: how robust the validation - doesnt have much variance when you the validation again - (find the robust percentage-is it really 10%)\n",
    "\n",
    "-Find whether training/validation split and k-folds is the best for kNN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: K-folds Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-folds Cross Validation is used as it uses all the data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1200 candidates, totalling 6000 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "\n",
    "# Creating a parameter of the grid\n",
    "param_grid = [{\n",
    "    'n_neighbors': list(range(1, 30, 2)),\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine', 'minkowski'],\n",
    "    'weights':['uniform','distance'],\n",
    "    #'algorithm':['auto','ball_tree','kd-tree','brute'],\n",
    "    'leaf_size' : list(range(1,50, 5))\n",
    "}]\n",
    "#is this how you put the hyperparameter range?\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "f1 = make_scorer(f1_score, pos_label = 1, average = 'binary')\n",
    "\n",
    "# Grid search model\n",
    "knn_grid = KNeighborsClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1)\n",
    "grid_search = GridSearchCV(estimator = knn_grid, param_grid = param_grid, cv =cv, verbose = 2, scoring = f1, n_jobs=-1)\n",
    "#go back and play around with this GridSearchCV parameter\n",
    "#from youtube: grid_search = GridSearchCV(estimator=knn,param_grid=knn_param, n_jobs=1, cv=cv, scoring=\"accuracy\", error_score=0)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is divided into k of 5 of equal parts (folds), and the model is trained and validated k times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding what is the best score/optimal accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding what is the best score/ optimal accuracy score\n",
    "print(\"Best F1-score: \",grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the hyperparameter to achieve this score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the hyperparameter to achieve this score\n",
    "print(\"The list of hyperparmeter in order to achieve this best score: \", grid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model with the best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = KNeighborsClassifier(n_neighbors=15, weights='uniform', leaf_size= 25, metric=\"manhattan\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(x_train,y_train)\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "#accuracy = best_model.score(x_train, y_train)\n",
    "#print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a validation of performance and confusion matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation and Confusion Matrix\n",
    "\n",
    "of the best model with the tuned hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize Result\n",
    "#precision,recall,f1-score,support, accuracy, macro avg, weighted avg\n",
    "print(classification_report(y_test,y_pred))\n",
    "#ROC score\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "#confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "#again print the best hyperparameter tuned for the best model\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can check if the accuracy had improve between the default_knn and when the hyperparameter is tuned for the best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(tuned_model, 'kNN.pkl', compress=9)\n",
    "\n",
    "#what is compress=9, do we need that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
