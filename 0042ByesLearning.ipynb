{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will compare the implementations of classic Naive Bayesian algorithms on the Adult dataset. There are multiple Naive Bayesian algorithms provided in scikit-learn library, including Guassin Naive Bayes, Multinomial Naive Bayes, Complement Naive Bayes, Bernoulli Naive Bayes and Categorical Naive Bayes. Here are the detailed descriptions for each type of Naive Bayes algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type|Description|Assumption|\n",
    "|--|--|--|\n",
    "|Guassin Naive Bayes (GaussianNB)|implements the Gaussian Naive Bayes algorithm for classification|Features are assumed to be Gaussian distributed|\n",
    "|Multinomial Naive Bayes (MultinomialNB)|classic naive Bayes variants used in text classification|Data is assumed to be multinomially distributed|\n",
    "|Complement Naive Bayes (ComplementNB)|An adaption of Multinomila Naive Bayes|Data is assumed to be multinomially distributed|\n",
    "|Bernoulli Naive Bayes (BernoulliNB)|implements the naive Bayes training and classification algorithms|Data is assumed to be multivariate Bernoulli distributed|\n",
    "|Categorical Naive Bayes (CategoricalNB)|implements the Categorical Naive Bayes algorithm for classification|Each feautre is assumed to be categorically distributed|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Adult dataset consists of 7 numerical features and 8 categorical features, and we aim at classifying people's income type using their census features. Hence, those algorithms (MultinomialNB, ComplementNB) specialized in text classification will not be included in this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv('./data_train.csv')\n",
    "data_train = data_train.reset_index()\n",
    "\n",
    "data_test = pd.read_csv('./data_test.csv')\n",
    "data_test = data_test.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all the categorical features\n",
    "categorical = [var for var in data_train.columns if data_train[var].dtype=='O']\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "print('The categorical variables are :\\n\\n', categorical)\n",
    "\n",
    "#Store all the numerical features\n",
    "numerical = [var for var in data_train.columns if data_train[var].dtype!='O']\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "print('The numerical variables are :\\n\\n', numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the feature columns and target column\n",
    "xs_train = data_train.drop(['income'], axis=1)\n",
    "ys_train = data_train['income']\n",
    "\n",
    "xs_test = data_test.drop(['income'], axis=1)\n",
    "ys_test = data_test['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the training set and validation set by 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xs_train_v, xs_val, ys_train_v, ys_val = train_test_split(xs_train, ys_train, test_size=0.2, random_state=42)\n",
    "\n",
    "xs_train_v = xs_train_v.reset_index()\n",
    "xs_val = xs_val.reset_index()\n",
    "\n",
    "ys_train_v = ys_train_v.reset_index()\n",
    "ys_val = ys_val .reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 For Guassian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GuassianNB(), we need to encode all the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gnb = ce.OneHotEncoder(cols=categorical)\n",
    "\n",
    "xs_gnb_train = encoder_gnb.fit_transform(xs_train_v)\n",
    "ys_gnb_train = ys_train_v['income']\n",
    "\n",
    "xs_gnb_val = encoder_gnb.transform(xs_val)\n",
    "ys_gnb_val = ys_val['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 For Catetgorical Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CategoricalNB(), we need to convert convert all the numerical features into categorical features by discretization. And they will be discretized by KBinsDiscretizer(), according to their histogram distribution showing in section EDA results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from matplotlib import pyplot\n",
    "\n",
    "age_d = xs_train_v['age']\n",
    "# reshape the Age column\n",
    "age_d=age_d.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "# define the kbinsdiscretizer for Age with 8 bins\n",
    "kbins = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform')\n",
    "age_trans = kbins.fit_transform(age_d)\n",
    "\n",
    "\n",
    "\n",
    "#process the Age column in validation set as well\n",
    "age_dv = xs_val['age']\n",
    "age_dv=age_dv.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform')\n",
    "age_trans_v = kbins.fit_transform(age_dv)\n",
    "\n",
    "# histogram of the transformed data\n",
    "pyplot.hist(age_trans_v, bins=8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 fnlwgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_d = xs_train_v['fnlwgt']\n",
    "# reshape the Final weight column\n",
    "fw_d=fw_d.values.reshape(-1,1)\n",
    "\n",
    "# define the kbinsdiscretizer for final weight with 8 bins\n",
    "kbins = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform')\n",
    "fw_trans = kbins.fit_transform(age_d)\n",
    "\n",
    "\n",
    "\n",
    "fw_dv = xs_val['fnlwgt']\n",
    "fw_dv=fw_dv.values.reshape(-1,1)\n",
    "\n",
    "#process the finalweight column in validation set as well\n",
    "kbins = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform')\n",
    "fw_trans_v = kbins.fit_transform(fw_dv)\n",
    "\n",
    "# histogram of the transformed data\n",
    "pyplot.hist(fw_trans_v, bins=8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 educational_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edunum_d = xs_train_v['educational_num']\n",
    "# reshape the educational num column\n",
    "edunum_d=edunum_d.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "# define the kbinsdiscretizer for educational num with 10 bins\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "edunum_trans = kbins.fit_transform(edunum_d)\n",
    "\n",
    "\n",
    "#process the educational num column in validation set as well\n",
    "edunum_dv = xs_val['educational_num']\n",
    "edunum_dv=edunum_dv.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "edunum_trans_v = kbins.fit_transform(edunum_dv)\n",
    "\n",
    "# histogram of the transformed data\n",
    "pyplot.hist(edunum_trans_v, bins=10)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 capital-gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_d = xs_train_v['capital-gain']\n",
    "# reshape the capital gain column\n",
    "cg_d=cg_d.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "# define the kbinsdiscretizer for capital gain with 10 bins\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "cg_trans = kbins.fit_transform(cg_d)\n",
    "\n",
    "#process the capital gain column in validation set as well\n",
    "cg_dv = xs_val['capital-gain']\n",
    "cg_dv=cg_dv.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "cg_trans_v = kbins.fit_transform(cg_dv)\n",
    "\n",
    "\n",
    "# histogram of the transformed data\n",
    "pyplot.hist(cg_trans_v, bins=10)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 capital-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following the similar process as above\n",
    "cl_d = xs_train_v['capital-loss']\n",
    "cl_d=cl_d.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "cl_trans = kbins.fit_transform(cl_d)\n",
    "\n",
    "cl_dv = xs_val['capital-loss']\n",
    "cl_dv=cl_dv.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "cl_trans_v = kbins.fit_transform(cl_dv)\n",
    "\n",
    "\n",
    "pyplot.hist(cl_trans_v, bins=10)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 hours-per-week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_d = xs_train_v['hours-per-week']\n",
    "hours_d=hours_d.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "hours_trans = kbins.fit_transform(hours_d)\n",
    "\n",
    "\n",
    "hours_dv = xs_val['hours-per-week']\n",
    "hours_dv=hours_dv.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "hours_trans_v = kbins.fit_transform(hours_dv)\n",
    "\n",
    "\n",
    "pyplot.hist(hours_trans_v, bins=10)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6 Combining converted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [4.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [3.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the array into dataframe, and combine them with those original categorical features\n",
    "age=pd.DataFrame(age_trans,columns =['age'])\n",
    "fw=pd.DataFrame(fw_trans,columns =['fnlwgt'])\n",
    "edunum=pd.DataFrame(edunum_trans,columns =['educational-num'])\n",
    "cg=pd.DataFrame(cg_trans,columns =['capital-gain'])\n",
    "cl=pd.DataFrame(cl_trans,columns =['capital-loss'])\n",
    "hours=pd.DataFrame(hours_trans,columns =['hours-per-week'])\n",
    "\n",
    "\n",
    "numerical_trans = pd.concat([age,fw,edunum,cg,cl,hours],axis=1)\n",
    "\n",
    "xs_cnb_train = pd.concat([xs_train_v[categorical],numerical_trans],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly for the validation dataset\n",
    "fw_v=pd.DataFrame(fw_trans_v,columns =['fnlwgt'])\n",
    "edunum_v=pd.DataFrame(edunum_trans_v,columns =['educational-num'])\n",
    "cg_v=pd.DataFrame(cg_trans_v,columns =['capital-gain'])\n",
    "cl_v=pd.DataFrame(cl_trans_v,columns =['capital-loss'])\n",
    "hours_v=pd.DataFrame(hours_trans_v,columns =['hours-per-week'])\n",
    "\n",
    "\n",
    "numerical_trans_v = pd.concat([age_v,fw_v,edunum_v,cg_v,cl_v,hours_v],axis=1)\n",
    "\n",
    "xs_cnb_val = pd.concat([xs_val[categorical],numerical_trans_v],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode all the remaining categorical features\n",
    "encoder_cnb = ce.OneHotEncoder(cols=categorical)\n",
    "\n",
    "xs_cnb_train = encoder_cnb.fit_transform(xs_cnb_train)\n",
    "xs_cnb_val = encoder_cnb.transform(xs_cnb_val)\n",
    "\n",
    "ys_cnb_train = ys_train_v['income']\n",
    "ys_cnb_val = ys_val['income']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 For Bernoulli Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Bernoulli Naive Bayes is suitable for classifying binary features, we can use the processed data for Categorical Naive Bayes here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_bnb_train = xs_cnb_train\n",
    "ys_bnb_train = ys_cnb_train\n",
    "\n",
    "xs_bnb_val = xs_cnb_val\n",
    "ys_bnb_val = ys_cnb_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Guassian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra columns cause by re-indexing\n",
    "xs_gnb_train = xs_gnb_train.drop(['level_0','index'],axis = 1)\n",
    "xs_gnb_val = xs_gnb_val.drop(['level_0','index'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(xs_gnb_train, ys_gnb_train)\n",
    "\n",
    "#Get prediction\n",
    "ys_gnb_pred = gnb.predict(xs_gnb_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validation for Guassian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "#Accuracy Score = (TP + TN)/ (TP + FN + TN + FP)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(ys_gnb_val, ys_gnb_pred)))\n",
    "\n",
    "print('Training set score: {:.4f}'.format(gnb.score(xs_gnb_train, ys_gnb_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(gnb.score(xs_gnb_val, ys_gnb_pred)))\n",
    "\n",
    "#Precision = TP/(TP+FP)\n",
    "precision = precision_score(ys_gnb_val, ys_gnb_pred)\n",
    "print('Model precision score: {:.4f}'.format(precision))\n",
    "\n",
    "#Recall = TP/(FN+TP)\n",
    "recall = recall_score(ys_gnb_val, ys_gnb_pred)\n",
    "print('Model recall score: {:.4f}'.format(recall))\n",
    "\n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, \n",
    "#where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('Model f1 score: {:.4f}'.format(f1_score(ys_gnb_val, ys_gnb_pred)))\n",
    "\n",
    "#ROC-AUC score\n",
    "print('Model ROC-AUC score: {:.4f}'.format(roc_auc_score(ys_gnb_val, ys_gnb_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Catetgorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "#Initialize categorical Naive Bayes\n",
    "cnb = CategoricalNB()\n",
    "\n",
    "#Fit the model\n",
    "cnb = cnb.fit(xs_cnb_train, ys_cnb_train)\n",
    "\n",
    "#Get prediction\n",
    "ys_cnb_pred = cnb.predict(xs_cnb_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Validation for Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score = (TP + TN)/ (TP + FN + TN + FP)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(ys_cnb_val, ys_cnb_pred)))\n",
    "\n",
    "print('Training set score: {:.4f}'.format(cnb.score(xs_cnb_train, ys_cnb_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(cnb.score(xs_cnb_val, ys_cnb_pred)))\n",
    "\n",
    "#Precision = TP/(TP+FP)\n",
    "precision = precision_score(ys_cnb_val, ys_cnb_pred)\n",
    "print('Model precision score: {:.4f}'.format(precision))\n",
    "\n",
    "#Recall = TP/(FN+TP)\n",
    "recall = recall_score(ys_cnb_val, ys_cnb_pred)\n",
    "print('Model recall score: {:.4f}'.format(recall))\n",
    "\n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, \n",
    "#where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('Model f1 score: {:.4f}'.format(f1_score(ys_cnb_val, ys_cnb_pred)))\n",
    "\n",
    "#AUC-ROC socre\n",
    "print('Model ROC-AUC score: {:.4f}'.format(roc_auc_score(ys_cnb_val, ys_cnb_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#Initialize Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "#Fit the model\n",
    "bnb.fit(xs_cnb_train,ys_cnb_train)\n",
    "\n",
    "#Get prediction\n",
    "ys_bnb_pred = bnb.predict(xs_bnb_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Validation for Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score = (TP + TN)/ (TP + FN + TN + FP)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(ys_bnb_val, ys_bnb_pred)))\n",
    "\n",
    "print('Training set score: {:.4f}'.format(bnb.score(xs_bnb_train, ys_bnb_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(bnb.score(xs_bnb_val, ys_bnb_pred)))\n",
    "\n",
    "#Precision = TP/(TP+FP)\n",
    "precision = precision_score(ys_bnb_val, ys_bnb_pred)\n",
    "print('Model precision score: {:.4f}'.format(precision))\n",
    "\n",
    "#Recall = TP/(FN+TP)\n",
    "recall = recall_score(ys_bnb_val, ys_bnb_pred)\n",
    "print('Model recall score: {:.4f}'.format(recall))\n",
    "\n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, \n",
    "#where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('Model f1 score: {:.4f}'.format(f1_score(ys_bnb_val, ys_bnb_pred)))\n",
    "\n",
    "#AUC-ROC score\n",
    "print('Model ROC-AUC score: {:.4f}'.format(roc_auc_score(ys_bnb_val, ys_bnb_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to a general comparison between those accuracy, precision, recall, f1 and ROC-AUC scores, CategoricalNB() performs better overall. Hence, the Categorical Naive Bayes cnb will be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Tesing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the unseen testing dataset to test the fincal chosen Categorical Naive Bayes model, cnb. However, we need to transform the testing dataset first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Testing data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Discretization the numerical features##\n",
    "\n",
    "#Age\n",
    "age_t = xs_test['age']\n",
    "age_t=age_t.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform')\n",
    "age_trans_t = kbins.fit_transform(age_t)\n",
    "\n",
    "#Final weight\n",
    "fw_t = xs_test['fnlwgt']\n",
    "fw_t=fw_t.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform')\n",
    "fw_trans_t = kbins.fit_transform(fw_t)\n",
    "\n",
    "#educational_num\n",
    "edunum_t = xs_test['educational_num']\n",
    "edunum_t=edunum_t.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "edunum_trans_t = kbins.fit_transform(edunum_t)\n",
    "\n",
    "#capital gain\n",
    "cg_t = xs_test['capital-gain']\n",
    "cg_t=cg_t.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "cg_trans_t = kbins.fit_transform(cg_t)\n",
    "\n",
    "#capital loss\n",
    "cl_t = xs_test['capital-loss']\n",
    "cl_t=cl_t.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "cl_trans_t = kbins.fit_transform(cl_t)\n",
    "\n",
    "#hours-per-week\n",
    "hours_t = xs_test['hours-per-week']\n",
    "hours_t=hours_t.values.reshape(-1,1)\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "hours_trans_t = kbins.fit_transform(hours_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_t=pd.DataFrame(age_trans_t,columns =['age'])\n",
    "fw_t=pd.DataFrame(fw_trans_t,columns =['fnlwgt'])\n",
    "edunum_t=pd.DataFrame(edunum_trans_t,columns =['educational-num'])\n",
    "cg_t=pd.DataFrame(cg_trans_t,columns =['capital-gain'])\n",
    "cl_t=pd.DataFrame(cl_trans_t,columns =['capital-loss'])\n",
    "hours_t=pd.DataFrame(hours_trans_t,columns =['hours-per-week'])\n",
    "\n",
    "\n",
    "numerical_trans_t = pd.concat([age_t,fw_t,edunum_t,cg_t,cl_t,hours_t],axis=1)\n",
    "\n",
    "xs_cnb_test = pd.concat([xs_test[categorical],numerical_trans_t],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same encoder all other categorical features\n",
    "xs_cnb_test = encoder_cnb.transform(xs_cnb_test)\n",
    "ys_cnb_test = ys_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model tesing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use unseen testing dataset to test the model cnb\n",
    "ys_cnb_pred_t = cnb.predict(xs_cnb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score = (TP + TN)/ (TP + FN + TN + FP)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(ys_cnb_test, ys_cnb_pred_t)))\n",
    "\n",
    "print('Training set score: {:.4f}'.format(cnb.score(xs_cnb_test, ys_cnb_test)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(cnb.score(xs_cnb_test, ys_cnb_pred_t)))\n",
    "\n",
    "#Precision = TP/(TP+FP)\n",
    "precision = precision_score(ys_cnb_test, ys_cnb_pred_t)\n",
    "print('Model precision score: {:.4f}'.format(precision))\n",
    "\n",
    "#Recall = TP/(FN+TP)\n",
    "recall = recall_score(ys_cnb_test, ys_cnb_pred_t)\n",
    "print('Model recall score: {:.4f}'.format(recall))\n",
    "\n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, \n",
    "#where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('Model f1 score: {:.4f}'.format(f1_score(ys_cnb_test, ys_cnb_pred_t)))\n",
    "\n",
    "print('Model ROC-AUC score: {:.4f}'.format(roc_auc_score(ys_cnb_test, ys_cnb_pred_t)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Results visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(cnb,xs_cnb_test, ys_cnb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(ys_cnb_test, ys_cnb_pred_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(cnb, xs_cnb_test, ys_cnb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(ys_cnb_test,ys_cnb_pred_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'CategoricalNaiveBayes.sav'\n",
    "pickle.dump(cnb, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the saved model, code in section 4.1 need to be run through to pre-process the data. And the, use the folowing code to load the Categorical Naive Bayes Model, cnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(xs_cnb_test, ys_cnb_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
